# -*- coding: utf-8 -*-
"""Crefit-card-fraud-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MWrgvOYPkS_nyet2r23O1_bFDThC7Nlq

#Requirements
"""

!pip install pandas scikit-learn kagglehub

"""#import"""

from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from sklearn.preprocessing import StandardScaler

import seaborn as sns
import matplotlib.pyplot as plt

from imblearn.over_sampling import SMOTE

import pandas as pd

import joblib

"""#Dataset Prepration"""

df = pd.read_csv('/content/creditcard.csv').fillna(0)

pd.options.display.max_columns = None
pd.options.display.max_rows = None

df.isnull().sum()

df.tail()

df.shape # rows and columns

df.info()

df.isnull().sum()

"""#Data Preprocssing"""

sc = StandardScaler()
df['Amount'] = sc.fit_transform(pd.DataFrame(df['Amount']))

df.head()

df.drop(['Time'], axis=1, inplace=True)

df.head()

df.shape

df.duplicated().any()

dup = df.drop_duplicates()
dup.shape

df['Class'].value_counts()

plt.style.use('ggplot')

sns.countplot(x='Class', data=df)
plt.title('Class Distributions')
plt.show()

"""#ML Alg"""

x = df.drop('Class', axis=1)
y = df['Class']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

classifiers = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

for name, clf in classifiers.items():
    print(f"Training {name}...")
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy of {name}: {accuracy}")
    print(f'Precision of {name}: {precision_score(y_test, y_pred)}')
    print(f'Recall of {name}: {recall_score(y_test, y_pred)}')
    print(f'F1 Score of {name}: {f1_score(y_test, y_pred)}')
    print()

# @title Default title text
# under sampling

normal = df[df['Class'] == 0]
fraud = df[df['Class'] == 1]

normal.shape

fraud.shape

normal_sample = normal.sample(n=fraud.shape[0])

# concatinate them
new_data = pd.concat([normal_sample, fraud],ignore_index=True)

new_data.head()

new_data['Class'].value_counts()

x = new_data.drop('Class', axis=1)
y = new_data['Class']

x_train , x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

classifiers = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}


for name, clf in classifiers.items():
    print(f"Training {name}...")
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy of {name}: {accuracy}")
    print(f'Precision of {name}: {precision_score(y_test, y_pred)}')
    print(f'Recall of {name}: {recall_score(y_test, y_pred)}')
    print(f'F1 Score of {name}: {f1_score(y_test, y_pred)}')
    print()

"""#Model Generation"""

rtc = RandomForestClassifier()
rtc.fit(x_train, y_train)

joblib.dump(rtc, 'random_forest_credit_card_model.pkl')

model = joblib.load('random_forest_credit_card_model.pkl')

test_data = [-2.3122265423263,1.95199201064158,-1.60985073229769,3.9979055875468,-0.522187864667764,-1.42654531920595,-2.53738730624579,1.39165724829804,-2.77008927719433,-2.77227214465915,3.20203320709635,-2.89990738849473,-0.595221881324605,-4.28925378244217,0.389724120274487,-1.14074717980657,-2.83005567450437,-0.0168224681808257,0.416955705037907,0.126910559061474,0.517232370861764,-0.0350493686052974,-0.465211076182388,0.320198198514526,0.0445191674731724,0.177839798284401,0.261145002567677,-0.143275874698919,0,]

a = model.predict([test_data])
a

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)

joblib.dump(dtc, 'credit_card_model.pkl')

model = joblib.load('credit_card_model.pkl')

"""#TestCase"""

# model.predict(x_test)

test_data = [-2.3122265423263,1.95199201064158,-1.60985073229769,3.9979055875468,-0.522187864667764,-1.42654531920595,-2.53738730624579,1.39165724829804,-2.77008927719433,-2.77227214465915,3.20203320709635,-2.89990738849473,-0.595221881324605,-4.28925378244217,0.389724120274487,-1.14074717980657,-2.83005567450437,-0.0168224681808257,0.416955705037907,0.126910559061474,0.517232370861764,-0.0350493686052974,-0.465211076182388,0.320198198514526,0.0445191674731724,0.177839798284401,0.261145002567677,-0.143275874698919,0,]

a = model.predict([test_data])

print(a)

if a==0:
    print("Normal Transaction")
else:
  print("Fraudulent Transaction")

test_data = [-2.3122265423263,1.95199201064158,-1.60985073229769,3.9979055875468,-0.522187864667764,-1.42654531920595,-2.53738730624579,1.39165724829804,-2.77008927719433,-2.77227214465915,3.20203320709635,-2.89990738849473,-0.595221881324605,-4.28925378244217,0.389724120274487,-1.14074717980657,-2.83005567450437,-0.0168224681808257,0.416955705037907,0.126910559061474,0.517232370861764,-0.0350493686052974,-0.465211076182388,0.320198198514526,0.0445191674731724,0.177839798284401,0.261145002567677,-0.143275874698919,0,]
a = model.predict([test_data])
print(a)
if a[0] == 0:
    print("Normal Transaction")
else:
  print("Fraudulent Transaction")

"""#Demo"""

# import kagglehub

# # Download latest version
# path = kagglehub.dataset_download("sriharshaeedala/financial-fraud-detection-dataset", "/content/")

# print("Path to dataset files:", path)

# data = pd.read_csv(path)

# print(data.head())
# print(data.info())

import kagglehub
import pandas as pd

data = pd.read_csv('/content/Synthetic_Financial_datasets_log.csv').fillna(0)

pd.options.display.max_columns = None
pd.options.display.max_rows = None

print(data.head())
print(data.info())

print(data.isnull().sum())

data.duplicated().any()

dup = data.drop_duplicates()
dup.shape

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

data = data.drop(['step', 'nameOrig', 'nameDest'], axis=1)

le = LabelEncoder()
data['type'] = le.fit_transform(data['type'])
# data.head()

data.head()

data.info()

X = data.drop(['isFraud', 'isFlaggedFraud'], axis=1)  # Drop both target columns for X
y = data[['isFraud', 'isFlaggedFraud']]  # Select both target columns for y

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print("Random Forest Accuracy: ", accuracy_score(y_test, y_pred))

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
print("Decision Tree Accuracy: ", accuracy_score(y_test, y_pred))

import joblib

joblib.dump(rf, 'random_forest_classification_credit_card_model.pkl')

model = joblib.load('random_forest_classification_credit_card_model.pkl')

ab = [3,9839.64,170136.0,160296.36,0.0,0.0]
cd = [1,181.00,181.0,0.00,21182.0,0.0]

print(le.classes_)
print(le.transform(le.classes_))

print()

pred = dt.predict([cd])
pred

model.predict([cd])

import joblib

model = joblib.load('random_forest_classification_credit_card_model.pkl')

ab = [3,9839.64,170136.0,160296.36,0.0,0.0]

cd = [1,181.00,181.0,0.00,21182.0,0.0]

result = model.predict([ab])

print(result)

# ['CASH_IN' 'CASH_OUT' 'DEBIT' 'PAYMENT' 'TRANSFER']
# [0 1 2 3 4]

"""#Demo 2"""

!pip install pandas numpy geopy scikit-learn

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from geopy.distance import geodesic

# Load dataset
df = pd.read_csv("/content/fraud_detection_dataset_5000.csv")

# Compute geographical movement distance
def calculate_distance(row):
    return geodesic(
        (row["location_latitude"], row["location_longitude"]),
        (row["last_location_latitude"], row["last_location_longitude"])
    ).kilometers

df["location_change_distance"] = df.apply(calculate_distance, axis=1)

# Device change indicator
df["device_changed"] = (df["device"] != df["last_device"]).astype(int)

# IP change indicator
df["ip_changed"] = (df["ip_address"] != df["last_ip"]).astype(int)

# Encode categorical features
label_enc = LabelEncoder()
df["device"] = label_enc.fit_transform(df["device"])
df["last_device"] = label_enc.transform(df["last_device"])

# Drop unnecessary columns
df.drop(columns=["user_id", "ip_address", "last_ip", "last_location_latitude", "last_location_longitude"], inplace=True)

# Feature Scaling
scaler = StandardScaler()
df[["amount", "failed_logins", "rapid_transactions", "location_change_distance"]] = scaler.fit_transform(
    df[["amount", "failed_logins", "rapid_transactions", "location_change_distance"]]
)

# Separate features and labels
X = df.drop(columns=["fraud_label"])
y = df["fraud_label"]

df.head(2)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv("/content/fraud_detection_dataset_5000.csv")

# Display first few rows
print("Dataset Preview:\n", df.head())


print("\nData Types:\n", df.dtypes)

# Check for missing values
print("\nMissing Values:\n", df.isnull().sum())

# Data distribution
df.hist(figsize=(10, 8), bins=30)
plt.suptitle("Data Distribution")
plt.show()

# Detect outliers using boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df.select_dtypes(include=[np.number]))
plt.title("Outliers Detection")
plt.xticks(rotation=90)
plt.show()

# Convert categorical columns to numeric for correlation analysis
df_encoded = df.copy()
for col in df.select_dtypes(include=['object']).columns:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])

# Correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(df_encoded.corr(), annot=True, cmap="coolwarm", fmt='.2f')
plt.title("Correlation Matrix")
plt.show()

# Line plot for pattern detection (select a numerical column)
if not df.select_dtypes(include=[np.number]).empty:
    plt.figure(figsize=(10, 5))
    sns.lineplot(data=df.select_dtypes(include=[np.number]))
    plt.title("Patterns in Data")
    plt.show()

# Data quality check: Checking duplicates
print("\nDuplicate Rows:", df.duplicated().sum())

# Summary statistics
print("\nSummary Statistics:\n", df.describe())

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Ensure X and y are defined properly before splitting
# Split data into training (80%) and testing (20%) sets with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize data for models sensitive to scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define classifiers with optimized hyperparameters
classifiers = {
    # 'Logistic Regression': LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42),
    # 'Decision Tree': DecisionTreeClassifier(max_depth=15, class_weight='balanced', random_state=42),
    # 'SVM': SVC(kernel='rbf', C=1.0, probability=True, random_state=42),
    # 'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    # 'Gradient Boosting': GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42),
    # 'XGBoost': XGBClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42, use_label_encoder=False, eval_metric='logloss')
}

best_model = None
best_f1_score = 0
best_model_name = ""

# Train and evaluate each classifier
for name, clf in classifiers.items():
    print(f"Training {name}...")

    # Scale data only for models that require it
    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:
        clf.fit(X_train_scaled, y_train)
        y_pred = clf.predict(X_test_scaled)
    else:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)

    # Calculate performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(f"\n{name} Performance:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

    # Store the best performing model
    if f1 > best_f1_score:
        best_f1_score = f1
        best_model = clf
        best_model_name = name

# Print the best model based on F1-score
print(f"\nBest Model: {best_model_name} with F1 Score: {best_f1_score:.4f}")

import joblib
joblib.dump(clf, "fraud_detection_model.pkl")

# Load model
model = joblib.load("fraud_detection_model.pkl")


# Index	Value	Feature Name	Description
# 0	1000	Transaction Amount	The amount of money (e.g., in USD) involved in the transaction. Higher amounts may have a higher risk of fraud.
# 1	40.8857	Location Latitude	The latitude of the transaction location.
# 2	-74.1868	Location Longitude	The longitude of the transaction location.
# 3	1	Failed Login Attempts	The number of failed login attempts before this transaction. More failed attempts may indicate a fraud attempt.
# 4	0	Rapid Transactions	Whether the user made multiple transactions in a short period (e.g., 1 for yes, 0 for no).
# 5	0	New Device Usage	Whether the transaction was made from a new device (1 for yes, 0 for no).
# 6	2	Distance Traveled	The distance (e.g., in kilometers) between this transaction and the last known transaction. Unusually large distances might be suspicious.
# 7	0.5	Transaction Hour Factor	A normalized value representing the time of the transaction. Certain hours (like late night) may have higher fraud risk.
# 8	0	IP Address Change	Whether the transaction was made from a different IP than the last login (1 for yes, 0 for no).
# 9	0	Device Change	Whether the device used for the transaction is different from the last device used (1 for yes, 0 for no).
# 10	0	Fraud Label (Optional)	Label indicating whether the transaction is fraudulent (1) or legitimate (0). This is only present in labeled data used for training.


new_transaction = np.array([[1000, 40.8857, -74.1868, 1, 0, 0, 2, 0.5, 0, 0, 0 ]])  # Example input with 11 features - replace with actual data

prediction = model.predict(new_transaction)

print("Fraud Prediction:", "Fraudulent" if prediction[0] == 1 else "Legitimate")

#import joblib
# def predict_transaction_type(model, transaction_data):
#     """Predict the type of credit card transaction using the loaded model."""
#     return model.predict([transaction_data])[0]
# # Load model
# model = joblib.load("fraud_detection_model.pkl")
# transaction_data = [1000, 40.8857, -74.1868, 1, 0, 0, 2, 0.5, 0, 0, 0 ]
# predicted_type = predict_transaction_type(model, transaction_data)
# print(predicted_type)  # Output: 'CASH_OUT'
# # ['CASH_IN', 'CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER']
# debug it

import joblib
import numpy as np

def predict_transaction_type(model, transaction_data):
    """Predict the type of credit card transaction using the loaded model."""
    # Reshape the input data to a 2D array as the model expects
    transaction_data = np.array([transaction_data])
    return model.predict(transaction_data)[0]

# Load model
model = joblib.load("fraud_detection_model.pkl")

transaction_data = [1000, 40.8857, -74.1868, 1, 0, 0, 2, 0.5, 0, 0, 0]
predicted_type = predict_transaction_type(model, transaction_data)

print(predicted_type)  # Output will be either 0 (legitimate) or 1 (fraudulent)

if predicted_type == 0:
    print("Normal Transaction")
else:
    print("Fraudulent Transaction")